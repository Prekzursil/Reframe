Hereâ€™s a first-pass project plan plus the four markdown docs you asked for (README, ARCHITECTURE, goal, TODO). You can copyâ€‘paste each block into its own file.

Iâ€™m calling the project **Reframe** (working title: *AI Media Toolkit*), but you can rename it easily.

---

````markdown
<!-- README.md -->

# Reframe â€“ AI Media Toolkit

> Localâ€‘first GUI app for AI shorts, captions, translation, and TikTokâ€‘style subtitles.

Reframe is a monolithic but modular toolkit that unifies all the experiments youâ€™ve collected:

- `ai-short-maker` (shorts + subtitles)
- `long_to_shorts_app` (FastAPI + Celery + GROQ)
- `video-subtitles-generator` (Descriptâ€‘style word highlighting)
- `subsai`, `Whisper-WebUI`, `pyvideotrans`, etc.

The goal is **one** desktopâ€‘friendly GUI that:

- turns long videos into shorts using AI,
- generates & translates captions,
- merges video + audio,
- translates SRTs,
- and burns *either* plain or TikTokâ€‘style wordâ€‘highlight subtitles.

Itâ€™s inspired by tools like Clipify (AI shorts from long videos), Subs AI (multiâ€‘backend Whisper UI), and pyVideoTrans (translation + dubbing pipeline). :contentReference[oaicite:0]{index=0}  

---

## Feature Overview

**Core v1 feature set**

- **AI Shorts Maker**
  - Input: local file or URL (YouTube / generic, via `yt-dlp`).
  - Modes:
    - *Auto interesting segments*: LLM ranks transcript chunks and picks top N moments. :contentReference[oaicite:1]{index=1}  
    - *Promptâ€‘guided*: â€œFind all moments where I talk about pricingâ€ etc.
  - Control: min/max clip length, number of clips, aspect ratio (9:16, 1:1, 16:9).
  - Output: rendered shorts (with or without burnt subtitles) + .srt/.ass + JSON metadata.

- **Caption & Translation**
  - Longâ€‘form captioning with **wordâ€‘level timestamps** (via whisperâ€‘timestamped / fasterâ€‘whisper / whisper.cpp). :contentReference[oaicite:2]{index=2}  
  - Export: `.srt`, `.vtt`, `.ass`, TXT.
  - Translate to target language(s) using pluggable translation backends.
  - Optional title/description translation for YouTube/TikTok upload workflows.

- **TikTokâ€‘style Subtitles**
  - Plain captions (classic SRT).
  - **Wordâ€‘byâ€‘word highlight** style similar to Descript / CapCut:
    - Bold white text with stroke & outline.
    - Perâ€‘word highlight color that appears exactly while the word is spoken. :contentReference[oaicite:3]{index=3}  
  - Styling presets: font, color, highlight color, outline, shadow, positioning.

- **SRT / Subtitle Translator**
  - Import `.srt` / `.ass`.
  - Translate while preserving timing & formatting.
  - Generate bilingual (stacked / sideâ€‘byâ€‘side) variants.

- **Video / Audio Merger**
  - Replace or mix audio track in a video with an external audio file.
  - Options: offset, duck original audio, normalize loudness.

- **â€œUtilitiesâ€ (future)**
  - Batch subtitling.
  - Silence trimming & pacing.
  - Speaker diarization for multiâ€‘speaker content (leveraging patterns from Whisper-WebUI & pyannote). :contentReference[oaicite:4]{index=4}  

---

## Highâ€‘Level Architecture

Reframe is designed as a **localâ€‘first monorepo**:

- `apps/api` â€“ Python FastAPI service exposing a JSON API.
- `apps/web` â€“ React + TypeScript frontend (can run as:
  - local web UI, and
  - desktop app via Tauri/Electron wrapper).
- `services/worker` â€“ Celery worker(s) for heavy media jobs.
- `packages/media-core` â€“ Python library with all media logic:
  - `transcribe/` â€“ Whisper, fasterâ€‘whisper, whisper.cpp, etc.
  - `segment/` â€“ longâ€‘toâ€‘short segmentation + LLM scoring.
  - `subtitles/` â€“ SRT/ASS generation, TikTokâ€‘style highlight via MoviePy.
  - `translate/` â€“ transcript & subtitle translation.
  - `video_edit/` â€“ clipping, scaling, merge, burnâ€‘in via FFmpeg/MoviePy.
  - `models/` â€“ pydantic models for jobs, media assets, subtitle styles.

Existing tools like Clipify, pyVideoTrans, and Subs AI are monolithic or semiâ€‘modular; Reframe explicitly separates **media core** from **API/UI**, so you can reuse the core from CLI tools or notebooks. :contentReference[oaicite:5]{index=5}  

See `ARCHITECTURE.md` for details.

---

## Tech Stack (Proposed)

**Backend / Media engine**

- Python 3.11+
- FastAPI (+ Uvicorn) for HTTP API.
- Celery + Redis (or RabbitMQ) for background jobs.
- FFmpeg for all audio/video I/O.
- Whisper variants (openai/whisper, fasterâ€‘whisper, whisper.cpp, whisperâ€‘timestamped).
- MoviePy + pysubs2 for subtitle rendering & styling.
- Pydantic / SQLModel + SQLite (dev) â†’ Postgres (optional, for server mode).

**Frontend**

- React + Vite + TypeScript.
- Shadcn/Radixâ€‘style component library (you already have this in `ai-short-maker`).
- Tailwind or CSSâ€‘inâ€‘JS (up to you).
- Optional: Tauri wrapper for native desktop app.

---

## How This Builds on Your Existing Prototypes

From the projects in `AI Media Toolkit/`:

- **`ai-short-maker`**
  - Great React UI patterns: sidebar layout, job queue, subtitle generator/translator forms.
  - Job model with status & log fields.
  - LLMâ€‘based transcript analysis using GROQ.
  - âœ Reuse the **UX patterns** and the idea of a `ProcessingJob` model, but reimplement core logic in `media-core`.

- **`long_to_shorts_app` (v1 + v2)**
  - FastAPI + Celery pattern for async video jobs.
  - Wholeâ€‘video transcription, equal sized clips, SRT+ASS generation with pysubs2.
  - Dockerâ€‘first deployment idea.
  - âœ Use this as the backbone for the new **FastAPI + Celery** layout and SRT/ASS generation.

- **`video-subtitles-generator`**
  - MoviePy pipeline for **perâ€‘word highlighted subtitles**.
  - Layered text (base text, outline, shadow, perâ€‘word highlight).
  - âœ Adopt this strategy inside `media-core/subtitles/highlighted.py` and expose style presets via the GUI.

- **`subsai`, `Whisper-WebUI`**
  - Demonstrate how to support multiple Whisper backends, VAD, and diarization in one UI. :contentReference[oaicite:6]{index=6}  
  - âœ Borrow config ideas (backend selection, model cache directory, device selection).

- **`pyvideotrans`**
  - Mature translation + dubbing pipeline, including SRT translation and audio reâ€‘synthesis. :contentReference[oaicite:7]{index=7}  
  - âœ Use as reference for future â€œfull translation + dubbingâ€ mode.

The point of Reframe is to **merge** these ideas into one consistent, testable architecture instead of having many oneâ€‘off experiments.

---

## Development Status

Right now this repo is in the **planning / scaffolding** stage:

- This README + `ARCHITECTURE.md` + `goal.md` describe the target system.
- `todo.md` contains a detailed checklist you can turn into issues.
- Initial work is:
  - scaffolding the monorepo,
  - wiring minimal FastAPI + worker + single â€œtranscribe video â†’ SRTâ€ endpoint,
  - creating a simple â€œupload video â†’ show subtitlesâ€ UI.

---

## Getting Started (planned flow)

Once the initial scaffolding is done, the flow will look like:

```bash
# 1. Start services
docker-compose up --build

# 2. API will run on http://localhost:8000
# 3. Web UI will run on http://localhost:5173 (or similar)
````

In development, youâ€™ll:

* Run FastAPI directly with `uvicorn apps.api.main:app --reload`.
* Start the worker with `celery -A services.worker worker -l info`.
* Run the React dev server with `npm run dev` in `apps/web`.

Exact commands are listed in `todo.md` once the layout is finalized.

---

## Roadmap

See [`todo.md`](./todo.md) for the detailed roadmap, grouped by:

* Project & Infra
* Media Core
* AI Shorts Maker
* Captioning & Translation
* Subtitle Styling
* SRT & Utilities
* Frontend UX
* Packaging & Distribution
* Observability & Testing

---

## License / Credits

Licensing TBD (MIT or Apacheâ€‘2 would keep things flexible), but the project is heavily **inspired by and will likely reuse ideas from**:

* Clipify ([GitHub][1])
* Subs AI ([GitHub][2])
* pyVideoTrans ([GitHub][3])
* The original `video-subtitles-generator` (Descriptâ€‘style subtitles)
* Your own `ai-short-maker` and `long_to_shorts_app` experiments.

Make sure to respect the original licenses of the code you copy or vendor in.

````

---

```markdown
<!-- ARCHITECTURE.md -->

# Reframe Architecture

## 1. Highâ€‘Level Overview

Reframe is a **localâ€‘first media processing stack**:

- **Frontend** (`apps/web`)
  - React SPA/MPA with routes for:
    - Shorts Maker
    - Caption & Translate
    - Subtitle Tools (SRT translator, style presets)
    - Utilities (video/audio merge)
    - Jobs / History
  - Can run:
    - in browser for local web UI, and
    - inside a Tauri/Electron shell for a desktop app.

- **API Service** (`apps/api`)
  - Python FastAPI app exposing a stable REST/JSON API.
  - Handles:
    - job creation & status polling,
    - file upload/download,
    - preset management (subtitle templates, model choices),
    - health checks.

- **Worker(s)** (`services/worker`)
  - Celery worker processes that run CPU/GPUâ€‘heavy tasks:
    - transcribing audio/video,
    - cutting clips,
    - rendering subtitled videos,
    - translating transcripts/SRT,
    - merging audio+video.

- **Media Core Library** (`packages/media-core`)
  - Pureâ€‘Python library with no FastAPI/React coupling.
  - Provides functions & classes for each step in the pipeline:
    - `Transcriber`
    - `Segmenter`
    - `SubtitleBuilder` (plain + TikTok highlight)
    - `Translator`
    - `VideoEditor`
  - Can be used standalone from CLI or notebooks.

- **Storage**
  - Local filesystem for:
    - uploaded media (`/data/media`),
    - intermediate audio (`/data/audio`),
    - transcripts & subtitles (`/data/subtitles`),
    - rendered outputs (`/data/outputs`).
  - SQLite DB (dev) for metadata & jobs; Postgres optional later.

---

## 2. Directory Layout (Proposed)

```text
.
â”œâ”€ apps/
â”‚  â”œâ”€ api/
â”‚  â”‚  â”œâ”€ main.py            # FastAPI entrypoint
â”‚  â”‚  â”œâ”€ routes/            # /shorts, /captions, /subtitles, /jobs
â”‚  â”‚  â”œâ”€ schemas/           # Pydantic request/response models
â”‚  â”‚  â””â”€ deps/              # DI, settings, DB session, etc.
â”‚  â””â”€ web/
â”‚     â”œâ”€ src/
â”‚     â”‚  â”œâ”€ pages/          # React routes (Shorts, Captions, etc.)
â”‚     â”‚  â”œâ”€ components/     # Forms, previews, job list, etc.
â”‚     â”‚  â””â”€ api/            # Typed API client
â”‚     â””â”€ ...
â”œâ”€ services/
â”‚  â””â”€ worker/
â”‚     â”œâ”€ worker.py          # Celery app
â”‚     â””â”€ tasks/             # Celery tasks using media-core
â”œâ”€ packages/
â”‚  â””â”€ media-core/
â”‚     â”œâ”€ transcribe/
â”‚     â”œâ”€ segment/
â”‚     â”œâ”€ subtitles/
â”‚     â”œâ”€ translate/
â”‚     â”œâ”€ video_edit/
â”‚     â”œâ”€ models/            # Pydantic models (Job, MediaAsset, SubtitleStyle, etc.)
â”‚     â””â”€ config.py
â”œâ”€ infra/
â”‚  â”œâ”€ docker-compose.yml
â”‚  â”œâ”€ Dockerfile.api
â”‚  â”œâ”€ Dockerfile.worker
â”‚  â””â”€ nginx.conf (optional)
â””â”€ docs/
   â”œâ”€ ARCHITECTURE.md
   â”œâ”€ README.md
   â”œâ”€ goal.md
   â””â”€ todo.md
````

---

## 3. Core Concepts & Data Models

### 3.1 Job

Represents a longâ€‘running processing task.

Fields (conceptual):

* `id: UUID`
* `type: Literal["shorts", "caption", "srt_translate", "merge_av"]`
* `status: Pending | Running | Completed | Failed | Cancelled`
* `input_media_ids: List[UUID]`
* `output_media_ids: List[UUID]`
* `params: dict` (serialized settings)
* `progress: float` (0â€“1)
* `logs: List[JobLogEntry]`
* `created_at`, `updated_at`

Jobs are created via API endpoints and executed by Celery workers.

### 3.2 MediaAsset

Represents a video/audio file or generated artifact.

* `id: UUID`
* `type: video | audio | srt | ass | vtt | json`
* `path: str` (relative to media root)
* `meta: dict` (codec info, length, language, etc.)
* `created_at`

### 3.3 Transcript & Subtitles

Standard internal representation:

```python
class Word(BaseModel):
    text: str
    start: float
    end: float
    confidence: float | None = None

class SubtitleLine(BaseModel):
    text: str
    start: float
    end: float
    words: list[Word]
```

* Output from **transcription** is `list[Word]`.
* `SubtitleBuilder` groups words into `SubtitleLine`s with configurable:

  * max chars per line,
  * max words per line,
  * max line duration,
  * max gap between words.

This follows the strategy used in tools like videoâ€‘subtitlesâ€‘generator (grouping wordâ€‘level timestamps into lineâ€‘level subtitles before rendering). ([GitHub][4])

### 3.4 SubtitleStyle

Defines styling for plain and TikTokâ€‘style subtitles:

```python
class SubtitleStyle(BaseModel):
    font_family: str
    font_size: int
    text_color: str          # hex
    highlight_color: str     # hex
    stroke_color: str
    stroke_width: int
    outline_color: str
    outline_width: int
    shadow_color: str
    shadow_offset: int
    position: Literal["bottom", "top", "center"]
    alignment: Literal["center", "left", "right"]
```

---

## 4. Pipelines

### 4.1 Shorts Maker Pipeline

**Use case:** Turn a long video into N sharable shorts with subtitles.

**Steps:**

1. **Ingest**

   * Frontend uploads a file or submits a URL.
   * API downloads remote URLs via `yt-dlp` and stores as `MediaAsset`.

2. **Transcribe**

   * Worker chooses STT backend (`whisper`, `faster-whisper`, `whisper.cpp`) based on config.
   * Produces `List[Word]` plus detected language.
   * Strategy similar to Subs AI / Whisper-WebUI multiâ€‘backend support. ([GitHub][2])

3. **Segment & Rank**

   * Build candidate segments by sliding over the timeline with windows (e.g. 15â€“90 seconds).
   * For each candidate:

     * Collect words & approximate emotion/structure features.
     * Send context + snippet to an LLM (Groq/OpenAI/etc.) with a scoring prompt:

       * *â€œRate how strong this is as a standalone short (1â€“10) and justify briefly.â€*
   * Select top N candidates subject to min/max clip length & minimum score.
   * This mirrors the approach described in â€œHow I Built an AIâ€‘Powered YouTube Shorts Generatorâ€. ([Vitalii Honchar][5])

4. **Clip Extraction**

   * Use FFmpeg to cut the video into selected segments.
   * Optionally reframe to 9:16 / 1:1 with crop/blur background.

5. **Subtitles (optional)**

   * Build `SubtitleLine`s from word timestamps per clip.
   * Choose style:

     * Plain SRT (no burnâ€‘in).
     * Burntâ€‘in plain text.
     * Burntâ€‘in TikTokâ€‘style word highlight (MoviePy).

6. **Export**

   * For each clip:

     * Save video file.
     * Save SRT/ASS.
     * Save JSON with metadata (title, language, transcript, chosen score).
   * Update `Job` with progress & asset references.

---

### 4.2 Caption & Translation Pipeline

**Use case:** Fullâ€‘length captions & translation for a long video/podcast.

1. **Transcribe**

   * As in Shorts pipeline but usually with a larger model or more accurate settings.
   * Optionally apply VAD or BGM separation for clearer speech (pattern from Whisper-WebUI). ([GitHub][4])

2. **Subtitle Grouping**

   * `SubtitleBuilder` groups words into lines with userâ€‘configurable rules.
   * Export `.srt`, `.vtt`, `.ass`.

3. **Translate (optional)**

   * Subtitle translation:

     * Parse SRT lines.
     * Call translation backend per line or batched.
     * Keep original timings.
   * Transcript translation:

     * For long content, chunk by paragraphs to keep context.
   * Backends can include:

     * Cloud APIs (DeepL, Google Translate, etc.).
     * Local models (e.g., Argos Translate, NLLB/M2M100 via HuggingFace). ([pyVideoTrans][6])

4. **Burnâ€‘In (optional)**

   * Use MoviePy or FFmpeg `subtitles` filter to burn SRT/ASS onto video.

---

### 4.3 TikTokâ€‘Style Highlight Subtitles

**Objective:** Wordâ€‘byâ€‘word highlighting synced to speech.

Implementation pattern (inspired by `video-subtitles-generator`):

1. Start from `SubtitleLine` with `words` and `start/end`.
2. For each line:

   * Calculate the width of each word via a temporary `TextClip`.
   * Position the line at bottom center by summing word widths.
3. Build layers:

   * **Shadow/outline layer** â€“ duplicate text at multiple offsets for deep outline.
   * **Base text layer** â€“ white (or configurable) text visible for the whole line duration.
   * **Highlight layer** â€“ perâ€‘word `TextClip`s:

     * `set_start(word.start)`
     * `set_duration(word.end - word.start)`
     * `color = highlight_color`.
4. Composite all clips onto the video or a colored background.
5. Export highâ€‘quality H.264/H.265 with audio.

This architecture gives you the Descript/CapCutâ€‘style karaoke effect with fine control over stroke, shadow, and color.

---

### 4.4 SRT / Subtitle Translator

* API endpoint accepts:

  * SRT/ASS file,
  * source & target language,
  * options (bilingual output, extra formatting).
* Worker:

  1. Parses file into `SubtitleLine`s.
  2. Groups text into batches for efficient translation.
  3. Calls translation backend.
  4. Reconstructs SRT/ASS, preserving timings and formatting tags where possible.
* Advanced options:

  * Keep original + translated stacked or sideâ€‘byâ€‘side.
  * Apply length constraints to avoid overflow.

---

### 4.5 Video / Audio Merger

Simple but very useful utility:

* Inputs:

  * Video file.
  * External audio file.
  * Options: start offset, duck original audio, normalize loudness.
* Implementation:

  * Use FFmpeg `-i video -i audio -filter_complex` to:

    * Align audio,
    * Mix (`amix`) with ducking,
    * Normalize (`loudnorm`),
    * Replace audio track if desired.
* Output:

  * New video with updated audio track.
  * Optional SRT/ASS burnâ€‘in in same pass.

---

## 5. API Surface (Draft)

Examples of key endpoints:

* `POST /api/v1/shorts/jobs`

  * Body: `ShortsJobCreate`
  * Response: `Job`

* `POST /api/v1/captions/jobs`

  * Body: `CaptionJobCreate`

* `POST /api/v1/subtitles/translate`

  * Upload SRT/ASS, choose target language(s).

* `POST /api/v1/utilities/merge-av`

  * Video + audio upload + options.

* `GET /api/v1/jobs/{job_id}`

  * Returns `Job` with progress & result assets.

* `GET /api/v1/assets/{asset_id}`

  * Streams file (video/audio/srt/etc.).

* `GET /api/v1/presets/subtitle-styles`

  * List builtâ€‘in & userâ€‘defined styles.

---

## 6. Extensibility

Reframe should be easy to extend:

* **New tools** are just new Celery tasks + API endpoints using `media-core`.
* **New STT backends** implement a `Transcriber` interface:

  * `transcribe(path, model, language) -> list[Word]`
* **New translation backends** implement a `Translator` interface:

  * `translate(texts: list[str], src: str, tgt: str) -> list[str]`
* **New subtitle styles** are configs; UI simply exposes them via sliders/pickers.

This mirrors how Subs AI and pyVideoTrans plug in multiple backends under a single interface. ([GitHub][2])

---

## 7. Deployment Modes

1. **Local Desktop (default)**

   * Everything runs on the userâ€™s machine.
   * GPU acceleration when available.
   * No auth, simple fileâ€‘based storage.

2. **Local Web Server**

   * Run in Docker on a machine with GPU.
   * Access via browser over LAN.
   * Optional basic auth.

3. **Hosted / Multiâ€‘User (future)**

   * Swap SQLite with Postgres.
   * Add user accounts, quotas, rate limits.
   * Optionally integrate S3â€‘style object storage.

---

## 8. Open Questions / Notes

* Which LLM provider to default to (Groq vs OpenAI vs local)?
* GPU vs CPU support matrix (fasterâ€‘whisper, whisper.cpp, etc.).
* Amount of configurability exposed in v1 vs hidden behind â€œAdvancedâ€ toggles.
* How to package MoviePy + FFmpeg reliably for Windows/macOS/Linux.

These are tracked as items in `todo.md`.

````

---

```markdown
<!-- goal.md -->

# Reframe â€“ Project Goals

## 1. Vision

Create a **single, localâ€‘first GUI app** that lets creators:

- turn long videos into engaging shorts,
- generate and translate captions,
- apply TikTokâ€‘style wordâ€‘highlight subtitles,
- and perform common subtitle/audio utilities,

without juggling a dozen separate scripts and repos.

---

## 2. Primary Use Cases

1. **Long video â†’ a batch of shorts**
   - Podcasters, educators, streamers repurposing 1â€“3 hour content.
   - Want: 5â€“20 shorts with good hooks, correct subtitles, and platformâ€‘ready formats.

2. **Fullâ€‘length captioning & translation**
   - Make longâ€‘form videos accessible and multilingual.
   - Export SRT/VTT for YouTube or burnâ€‘in subtitles directly.

3. **Stylized subtitles for social**
   - Generate *Descript/CapCutâ€‘style* wordâ€‘byâ€‘word highlighted captions in a few clicks.

4. **SRT translation & cleanup**
   - Take existing subtitle files, translate them, keep timing, maybe generate bilingual versions.

5. **Audio replacement / merging**
   - Replace camera audio with a mastered track.
   - Merge translated/dubbed audio back into the original.

---

## 3. Nonâ€‘Goals (for now)

- Full, productionâ€‘grade video editor (timeline, transitions, color grading).
- Fully automated multiâ€‘platform uploader (YouTube/TikTok API integration).
- Realâ€‘time streaming / live captioning (this is strictly offline/batch for now).
- Deepfake voice cloning â€“ translation/dubbing will initially rely on basic TTS or external tools.

---

## 4. Phase Plan

### Phase 0 â€“ Consolidate & Scaffold

**Goal:** Turn the existing experiments into a unified skeleton.

- Decide & lock in tech stack (FastAPI + Celery + React + media-core).
- Scaffold monorepo (`apps/api`, `apps/web`, `packages/media-core`, `services/worker`, `infra`).
- Port the *simplest* feature endâ€‘toâ€‘end:
  - video upload â†’ transcribe â†’ SRT download.

Success criteria:

- You can run `docker-compose up` and get:
  - API docs at `/docs`,
  - A simple web form to upload a video and get SRT.

---

### Phase 1 â€“ Captions & SRT Translation

**Goal:** Reliable transcription + subtitle translation, no shorts yet.

- Implement:
  - multiâ€‘backend transcription (whisper, fasterâ€‘whisper, whisper.cpp),
  - `SubtitleBuilder` with grouping rules,
  - SRT/VTT/ASS export,
  - SRT translator (single â†’ target language).

- Frontend:
  - â€œCaptions & Translateâ€ page with:
    - upload video,
    - choose language & model,
    - progress view,
    - downloadable files list.

Success criteria:

- You can feed in a 30â€“60 minute video and get usable subtitles + a translated SRT, via the GUI.

---

### Phase 2 â€“ TikTokâ€‘Style Subtitles

**Goal:** Nail the â€œwowâ€ factor: wordâ€‘byâ€‘word highlighting.

- Implement MoviePyâ€‘based renderer with:
  - perâ€‘word highlight overlays,
  - configurable stroke, outline, shadow, colors.
- Add subtitle style presets and a visual preview in the UI.
- Allow exporting:
  - just styled video,
  - and SRT/ASS alongside it.

Success criteria:

- Given a transcript + video, you can generate a vertical video with TikTokâ€‘style captions and tweak style in the GUI.

---

### Phase 3 â€“ AI Shorts Maker

**Goal:** Highâ€‘quality shorts from long videos.

- Implement segmentation & ranking:
  - sliding windows over transcript,
  - LLM scoring, with prompt customization.
- Expose parameters:
  - number of clips,
  - min/max duration,
  - aspect ratio,
  - whether to add subtitles & style.
- Integrate job queue & progress UI.

Success criteria:

- Short creators can feed a 1+ hour video and get a batch of shorts with captions in one run.

---

### Phase 4 â€“ Utilities & Polish

**Goal:** Round out the app and make it pleasant to live in.

- Audio/video merge tool.
- SRT editor basics (fix text inline, reâ€‘time lines).
- Preset management (subtitle styles, language presets).
- Desktop packaging (Tauri/Electron).
- Basic logging, error handling, and crash reporting.

Success criteria:

- You can use Reframe as your default â€œAI subtitle + shortsâ€ toolbox without touching scripts.

---

## 5. Success Metrics

Qualitative:

- You stop switching between your old tools for 80â€“90% of subtitle + shorts work.
- Generating subtitles or shorts is fast enough to be used in real workflows (minutes, not hours, on typical hardware).

Quantitative (once youâ€™re dogfooding):

- Time from â€œdrop in a long videoâ€ to â€œhave a batch of shorts + captionsâ€ reduced by at least **3Ã—** vs your current workflow.
- You can run multiple jobs per day without major babysitting.

---

## 6. Guiding Principles

- **Localâ€‘first** by default; no mandatory cloud dependency.
- **Pluggable** for models & APIs (STT, LLM, translation).
- **One mental model**: upload â†’ pick tool â†’ run job â†’ see outputs.
- **Plain data structures** internally (words, lines, clips) so scripts & notebooks can reuse the core easily.
````

---

```markdown
<!-- todo.md -->

# TODO / Roadmap

Below is a structured checklist you can turn into GitHub issues.

---

## 1. Project & Infra

- [ ] Create monorepo layout: `apps/`, `services/`, `packages/`, `infra/`.
- [ ] Add `pyproject.toml` for `packages/media-core` (Poetry or plain pip).
- [ ] Add `apps/api/` with FastAPI skeleton + `main.py`.
- [ ] Add `apps/web/` with React + Vite + TypeScript.
- [ ] Add `services/worker/` with Celery app.
- [ ] Add `infra/docker-compose.yml` for API, worker, Redis, web.
- [ ] Add Dockerfiles: `Dockerfile.api`, `Dockerfile.worker`, `Dockerfile.web`.
- [ ] Add root `.gitignore` for Python, Node, env, media, build artifacts.
- [ ] Add `.env.example`:
  - [ ] API: `DATABASE_URL`, `MEDIA_ROOT`, `BROKER_URL`, `RESULT_BACKEND`, `OPENAI_API_KEY`, `GROQ_API_KEY`, `TRANSLATOR_*`.
  - [ ] Web: `VITE_API_BASE_URL`.
- [ ] Add basic `Makefile` or task runner (`justfile`) for common commands.
- [ ] Add preâ€‘commit config (ruff/black/isort, eslint/prettier).

---

## 2. Media Core â€“ Transcription

- [ ] Create `packages/media-core/transcribe/__init__.py`.
- [ ] Implement `TranscriptionConfig` (model, language, device, backend).
- [ ] Implement `Word` and `TranscriptionResult` models.
- [ ] Backend: `openai_whisper` (simple baseline).
- [ ] Backend: `faster_whisper` (GPUâ€‘friendly).
- [ ] Backend: `whisper_cpp` integration (via `pywhispercpp` or subprocess).
- [ ] Optional: support `whisper-timestamped` or `whisperX` for more accurate word timings.
- [ ] Normalize outputs to `List[Word]` regardless of backend.
- [ ] Add CLI entrypoint (`python -m media_core.transcribe`) for quick testing.
- [ ] Unit tests: transcription result normalization (words sorted, no overlaps, correct lengths).

---

## 3. Media Core â€“ Subtitle Building

- [ ] Create `packages/media-core/subtitles/builder.py`.
- [ ] Implement `SubtitleLine` model (with `words: list[Word]`).
- [ ] Implement grouping logic:
  - [ ] `max_chars_per_line`,
  - [ ] `max_words_per_line`,
  - [ ] `max_duration`,
  - [ ] `max_gap`.
- [ ] Export to SRT writer.
- [ ] Export to VTT writer.
- [ ] Export to ASS writer (basic).
- [ ] Use `pysubs2` for ASS styling where helpful.
- [ ] Unit tests: grouping for different languages & fast/slow speech.

---

## 4. Media Core â€“ TikTokâ€‘Style Renderer

- [ ] Create `packages/media-core/subtitles/styled.py`.
- [ ] Implement `SubtitleStyle` model (font, colors, stroke, shadow, outline, position).
- [ ] Implement `StyledSubtitleRenderer` using MoviePy:
  - [ ] Function to compute word sizes/positions given frame size.
  - [ ] Base text layer (full line duration).
  - [ ] Shadow/outline layers.
  - [ ] Perâ€‘word highlight `TextClip`s with wordâ€‘specific `start/end`.
- [ ] Support variable video resolutions & aspect ratios (auto center).
- [ ] Support vertical (9:16) & horizontal (16:9) layouts.
- [ ] Add a simple â€œsolid background + subtitles onlyâ€ mode for preview.
- [ ] Provide a few preset styles (e.g. â€œTikTok defaultâ€, â€œYellow highlightâ€, â€œClean whiteâ€).
- [ ] Integration test: render a 5â€“10 second sample with 3 lines and verify no crashes.

---

## 5. Media Core â€“ Translation

- [ ] Create `packages/media-core/translate/__init__.py`.
- [ ] Define `Translator` interface:
  - [ ] `translate_batch(texts: list[str], src: str, tgt: str) -> list[str]`.
- [ ] Implement simple cloud translation backend (if you already use one).
- [ ] Implement local/offline backend (e.g., Argos Translate / HF model) where feasible.
- [ ] Implement SRT translator:
  - [ ] Parse SRT â†’ list of `SubtitleLine`.
  - [ ] Batch lines for translation.
  - [ ] Rebuild SRT while preserving timings.
- [ ] Implement bilingual SRT builder (original + translated lines).
- [ ] Unit tests: translation preserves count/order, handles empty lines.

---

## 6. Media Core â€“ Video Editing

- [ ] Create `packages/media-core/video_edit/ffmpeg.py`.
- [ ] Function: `probe_media(path) -> dict` (duration, resolution, codecs).
- [ ] Function: `extract_audio(video_path, audio_path)`.
- [ ] Function: `cut_clip(video_path, start, end, output_path)`.
- [ ] Function: `reframe(video_path, output_path, aspect_ratio, strategy="crop|blur_bg")`.
- [ ] Function: `merge_video_audio(video_path, audio_path, output_path, offset, ducking, normalize)`.
- [ ] Function: `burn_subtitles(video_path, srt_or_ass_path, output_path, extra_filters=None)`.
- [ ] Tests: basic FFmpeg invocation works and outputs exist.

---

## 7. Media Core â€“ Shorts Segmentation

- [ ] Create `packages/media-core/segment/shorts.py`.
- [ ] Define `SegmentCandidate` model (start, end, score, reason, snippet).
- [ ] Implement naive equalâ€‘splits strategy (baseline, from `long_to_shorts_app`).
- [ ] Implement sliding window candidate generator (configurable window size & stride).
- [ ] Implement scoring using simple heuristics (density of keywords, sentence boundaries).
- [ ] Implement LLM scoring backend:
  - [ ] Interface: `score_segments(transcript, candidates, prompt, model)`.
  - [ ] Provider: Groq or OpenAI (whichever you prefer).
- [ ] Implement selector: pick top N segments under min/max duration & nonâ€‘overlap rules.
- [ ] Unit tests: segments nonâ€‘overlapping, durations within bounds.

---

## 8. Worker Service (Celery)

- [ ] Set up `services/worker/worker.py` with Celery app initialization.
- [ ] Configure broker/result backend via env (Redis by default).
- [ ] Task: `transcribe_video(video_asset_id, config) -> transcription_asset_id`.
- [ ] Task: `generate_captions(video_asset_id, options) -> srt_asset_id`.
- [ ] Task: `translate_subtitles(subtitle_asset_id, options) -> new_subtitle_asset_id`.
- [ ] Task: `render_styled_subtitles(video_asset_id, subtitle_asset_id, style, options) -> video_asset_id`.
- [ ] Task: `generate_shorts(video_asset_id, options) -> list[clip_asset_id]`.
- [ ] Task: `merge_video_audio(video_asset_id, audio_asset_id, options) -> video_asset_id`.
- [ ] Implement job status updates & progress reporting.

---

## 9. API â€“ Core

- [ ] Implement settings management using `pydantic-settings`.
- [ ] Add DB models (SQLModel or SQLAlchemy) for:
  - [ ] `Job`,
  - [ ] `MediaAsset`,
  - [ ] (Optional) `SubtitleStylePreset`.
- [ ] Add migration tooling (Alembic) if using SQLAlchemy.
- [ ] Endpoint: `POST /api/v1/captions/jobs`.
- [ ] Endpoint: `POST /api/v1/subtitles/translate`.
- [ ] Endpoint: `POST /api/v1/shorts/jobs`.
- [ ] Endpoint: `POST /api/v1/utilities/merge-av`.
- [ ] Endpoint: `GET /api/v1/jobs/{job_id}`.
- [ ] Endpoint: `GET /api/v1/jobs` (listing/filtering).
- [ ] Endpoint: `GET /api/v1/assets/{asset_id}` (download).
- [ ] Endpoint: `GET /api/v1/presets/styles`.
- [ ] Add OpenAPI docs with tags & examples.

---

## 10. API â€“ Job Lifecycle & Errors

- [ ] Standardize job statuses & error codes.
- [ ] Add structured error responses (code, message, details).
- [ ] Add background cleanup for orphaned temp files.
- [ ] Add endpoint to cancel a running job (best effort).
- [ ] Add rate limiting for heavy endpoints (optional, later).

---

## 11. Frontend â€“ Shell & Shared

- [ ] Scaffold layout:
  - [ ] Sidebar or top nav with sections: Shorts, Captions, Subtitles, Utilities, Jobs.
  - [ ] Shared header/footer.
- [ ] Add base UI kit (buttons, inputs, selects, modals, toasts).
- [ ] Add global loading spinner and error boundary.
- [ ] Implement typed API client (axios/fetch with TS types).
- [ ] Configure theme (dark/light) with CSS variables or Tailwind.
- [ ] Add simple settings modal (default model, language, output paths, etc.).

---

## 12. Frontend â€“ Captions & Translate

- [ ] Page: **Captions & Translate**.
- [ ] Section: Upload video / dropzone.
- [ ] Form controls:
  - [ ] Source language (auto / manual).
  - [ ] Whisper backend & model selection.
  - [ ] Output formats (SRT/VTT/ASS).
  - [ ] Target language(s) for translation (optional).
- [ ] On submit:
  - [ ] Create caption job via API.
  - [ ] Show job in â€œRecent jobsâ€ panel with status & progress.
- [ ] When job completes:
  - [ ] Show download buttons for each generated asset.
  - [ ] Preview subtitles in a simple video player if possible.

---

## 13. Frontend â€“ TikTokâ€‘Style Subtitles

- [ ] Page: **Subtitle Styling**.
- [ ] UI:
  - [ ] Upload video OR select an existing `MediaAsset`.
  - [ ] Select subtitles (existing SRT) OR generate from scratch (reusing captions pipeline).
  - [ ] Style editor:
    - [ ] Font family (dropdown).
    - [ ] Font size slider.
    - [ ] Text color picker.
    - [ ] Highlight color picker.
    - [ ] Stroke width slider.
    - [ ] Outline toggle + width slider + color picker.
    - [ ] Shadow toggle + offset slider.
    - [ ] Position & alignment controls.
  - [ ] â€œPreview 5 secondsâ€ button to render a short preview.
- [ ] â€œRender full videoâ€ button -> creates a styled subtitle job.

---

## 14. Frontend â€“ AI Shorts Maker

- [ ] Page: **Shorts Maker**.
- [ ] Input:
  - [ ] Video upload or URL input.
  - [ ] Number of clips desired.
  - [ ] Min/max clip duration.
  - [ ] Aspect ratio selection.
  - [ ] â€œUse subtitlesâ€ toggle with style selector.
  - [ ] â€œPrompt to guide selectionâ€ textarea.
- [ ] Submit:
  - [ ] Create shorts job.
  - [ ] Show a progress view with steps (transcribe â†’ segment â†’ render).
- [ ] Result view:
  - [ ] Grid of generated clips with:
    - [ ] Thumbnail / GIF.
    - [ ] Duration.
    - [ ] Score.
    - [ ] Download buttons (video + subtitles).
  - [ ] Ability to delete/ignore clips.

---

## 15. Frontend â€“ Utilities (SRT & Merge)

- [ ] Page: **Subtitle Tools**.
  - [ ] SRT upload â†’ translation options â†’ result download.
  - [ ] Bilingual SRT option.
- [ ] Page: **Video / Audio Merge**.
  - [ ] Upload/choose video.
  - [ ] Upload/choose audio.
  - [ ] Controls: offset, ducking, normalize.
  - [ ] Submit â†’ job â†’ result download.

---

## 16. Frontend â€“ Jobs & History

- [ ] Page: **Jobs**.
  - [ ] Table listing with filters (status, type, date).
  - [ ] Each row shows progress bar and link to result view.
- [ ] Job detail:
  - [ ] Show inputs, outputs, logs.
  - [ ] Actions: download all as zip, copy transcript, etc.

---

## 17. Observability & Testing

- [ ] Integrate structured logging on the backend (JSON logs).
- [ ] Log FFmpeg commands and exit codes when processing fails.
- [ ] Add health check endpoint (`/healthz`).
- [ ] Unit tests for media-core modules:
  - [ ] transcribe, subtitles, translate, video_edit, segment.
- [ ] Integration tests:
  - [ ] Endâ€‘toâ€‘end â€œvideo â†’ SRTâ€ job.
  - [ ] Endâ€‘toâ€‘end â€œvideo â†’ TikTokâ€‘style renderedâ€ sample.
  - [ ] Endâ€‘toâ€‘end â€œvideo â†’ shorts with subtitlesâ€ with small test video.
- [ ] Frontend tests:
  - [ ] Component tests for forms and job list.
  - [ ] Minimal e2e flow (upload â†’ job complete â†’ download).

---

## 18. Packaging & Distribution

- [ ] Add `Dockerfile` for an â€œallâ€‘inâ€‘oneâ€ image (API + worker) for simple servers.
- [ ] Tauri/Electron:
  - [ ] Decide wrapper (Tauri recommended for performance).
  - [ ] Wire Tauri to run API/worker as child processes or rely on local Docker.
  - [ ] Integrate update mechanism (optional later).
- [ ] Provide example configs for:
  - [ ] Local dev (no GPU),
  - [ ] Local GPU workstation,
  - [ ] Small server deployment.

---

## 19. UX & Polish

- [ ] Add onboarding â€œwizardâ€ or quick start card on home page.
- [ ] Provide a few sample videos for testing.
- [ ] Add warnings for long jobs (e.g., â€œthis may take ~N minutes on CPUâ€).
- [ ] Expose only the most important knobs in v1; hide advanced settings in collapsible sections.
- [ ] Add tooltips explaining Whisper backends, models, and tradeâ€‘offs.
- [ ] Add â€œcopy commandâ€ buttons that show the equivalent CLI for advanced users.

---

## 20. Future / Niceâ€‘toâ€‘Have

- [ ] Speaker diarization integration (pyannote) for speakerâ€‘labeled subtitles.
- [ ] Smart silence trimming (cut dead air before generating shorts).
- [ ] Basic subtitle editor (inline text edit + shift timings).
- [ ] Support for timelines / EDL export.
- [ ] Optional cloud integrations (S3, remote GPU workers).
- [ ] Optional â€œexport upload packageâ€ for YouTube/TikTok (title, description, tags).

```

---

If youâ€™d like, next step I can help you:

* adapt the `ai-short-maker` React UI to this new API shape, or
* sketch actual Pydantic models & FastAPI route signatures so you can start coding immediately.

[1]: https://github.com/Automately-Solutions/Clipify?utm_source=chatgpt.com "Clipify: Transform Long Videos into Engaging Short Clips"
[2]: https://github.com/absadiki/subsai?utm_source=chatgpt.com "absadiki/subsai: ğŸï¸ Subtitles generation tool (Web-UI + ..."
[3]: https://github.com/jianchang512/pyvideotrans?utm_source=chatgpt.com "jianchang512/pyvideotrans: Translate the video from one ..."
[4]: https://github.com/sindresorhus/awesome-whisper?utm_source=chatgpt.com "Awesome list for Whisper â€” an open-source AI-powered ..."
[5]: https://vitaliihonchar.com/insights/youtube-shorts-creator?utm_source=chatgpt.com "How I Built an AI-Powered YouTube Shorts Generator"
[6]: https://pyvideotrans.com/?utm_source=chatgpt.com "pyVideoTrans-Open Source Video Translation Tool ..."
